% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_WRMF.R
\docType{data}
\name{WRMF}
\alias{WRMF}
\title{(Weighted) Regularized Matrix Facrtorization for collaborative filtering}
\format{\code{R6Class} object.}
\usage{
WRMF
}
\description{
Creates matrix factorization model which could be solved with Alternating Least Squares (Weighted ALS for implicit feedback).
For implicit feedback see (Hu, Koren, Volinsky)'2008 paper \url{http://yifanhu.net/PUB/cf.pdf}.
For explicit feedback model is classic model for rating matrix decomposition with MSE error (without biases at the moment).
These two algorithms are proven to work well in recommender systems.
}
\section{Usage}{

For usage details see \bold{Methods, Arguments and Examples} sections.
\preformatted{
  model = WRMF$new(rank = 10L, lambda = 0,
                  feedback = c("implicit", "explicit"),
                  init_stdv = 0.01,
                  n_threads = parallel::detectCores(),
                  non_negative = FALSE,
                  solver = c("conjugate_gradient", "cholesky"),
                  cg_steps = 3L,
                  components = NULL)
  model$fit_transform(x, n_iter = 5L, ...)
  model$predict(x, k, not_recommend = x, ...)
  model$components
  model$add_scorers(x_train, x_cv, specs = list("map10" = "map@10"), ...)
  model$remove_scorer(name)
}
}

\section{Methods}{

\describe{
  \item{\code{$new(rank = 10L, lambda = 0, feedback = c("implicit", "explicit"),
                   init_stdv = 0.01, n_threads = parallel::detectCores(), non_negative = FALSE,
                   solver = c("conjugate_gradient", "cholesky"), cg_steps = 3L,
                   components = NULL) }}{ creates matrix
    factorization model model with \code{rank} latent factors. If \code{components} is provided then initialize
    item embeddings with its values.}
  \item{\code{$fit_transform(x, n_iter = 5L, ...)}}{ fits model to
    an input user-item matrix. (preferably in "dgCMatrix" format).
    For implicit feedback \code{x} should be a confidence matrix which corresponds to \code{1 + alpha * r_ui} in original paper.
    Usually \code{r_ui} corresponds to the number of interactions of user \code{u} and item \code{i}.
    For explicit feedback values in \code{x} represents ratings.
    \bold{Returns factor matrix for users of size \code{n_users * rank}}}
  \item{\code{$predict(x, k, not_recommend = x, ...)}}{predict \code{top k}
    item ids for users \code{x} (= column names from the matrix passed to \code{fit_transform()} method).
    Users features should be defined the same way as they were defined in training data - as \bold{sparse matrix}
    of confidence values (implicit feedback) or ratings (explicit feedback).
    Column names (=item ids) should be in the same order as in the \code{fit_transform()}.}
  \item{\code{$add_scorers(x_train, x_cv, specs = list("map10" = "map@10"), ...)}}{add a metric to watchlist.
  Metric will be evaluated after each ALS interation. At the moment following metrices are supported:
    \bold{"loss"}, \bold{"map@k"}, \bold{"ndcg@k"}, where \bold{k} is some integer. For example \code{map@10}.}
  \item{\code{$remove_scorer(name)}}{remove a metric from watchlist}
  \item{\code{$components}}{item factors matrix of size \code{rank * n_items}}
  \item{n_threads}{\code{numeric} default number of threads to use during training and prediction
  (if OpenMP is available).}
}
}

\section{Arguments}{

\describe{
 \item{model}{A \code{WRMF} model.}
 \item{x}{An input sparse user-item matrix(of class \code{dgCMatrix}).
 For explicit feedback should consists of ratings.
 For implicit feedback all positive interactions should be filled with \bold{confidence} values.
 Missed interactions should me zeros/empty.
 So for simple case case when \code{confidence = 1 + alpha * x}}
 \item{x_train}{An input user-item \bold{relevance} matrix. Used during evaluation of \code{map@k}, \code{ndcg@k}
   Should have the same shape as corresponding confidence matrix \code{x_cv}.
   Values are used as "relevance" in ndgc calculation}
 \item{x_cv}{user-item matrix used for validation (ground-truth observations)}
 \item{name}{\code{character} - user-defined name of the scorer. For example "ndcg-scorer-1"}
 \item{rank}{\code{integer} - number of latent factors}
 \item{lambda}{\code{numeric} - regularization parameter}
 \item{feedback}{\code{character} - feedback type - one of \code{c("implicit", "explicit")}}
 \item{solver}{\code{character} - solver for "implicit feedback" problem.
    One of \code{c("conjugate_gradient", "cholesky")}.
    Usually approximate \code{"conjugate_gradient"} is significantly faster and solution is
    on par with exact \code{"cholesky"}}
 \item{cg_steps}{\code{integer > 0} - max number of internal steps in conjugate gradient
    (if "conjugate_gradient" solver used). \code{cg_steps = 3} by default.
    Controls precision of linear equation solution at the each ALS step. Usually no need to tune this parameter.}
 \item{preprocess}{\code{function} = \code{identity()} by default. User spectified function which will be applied to user-item interaction matrix
    before running matrix factorization (also applied in inference time before making predictions). For example we may
    want to normalize each row of user-item matrix to have 1 norm. Or apply \code{log1p()} to discount large counts.
    This essentially corresponds to the "confidence" function from (Hu, Koren, Volinsky)'2008 paper \url{http://yifanhu.net/PUB/cf.pdf}}
 \item{n_threads}{\code{numeric} default number of threads to use during training and prediction
 (if OpenMP is available).}
 \item{not_recommend}{\code{sparse matrix} or \code{NULL} - points which items should be excluided from recommendations for a user.
   By default it excludes previously seen/consumed items.}
 \item{convergence_tol}{{\code{numeric = -Inf} defines early stopping strategy. We stop fitting
    when one of two following conditions will be satisfied: (a) we have used
    all iterations, or (b) \code{loss_previous_iter / loss_current_iter - 1 < convergence_tol}}}
 \item{init_stdv}{\code{numeric} standart deviation for initialization of the initial latent matrices}
 \item{...}{other arguments. Not used at the moment}
}
}

\seealso{
\itemize{
  \item{\url{https://math.stackexchange.com/questions/1072451/analytic-solution-for-matrix-factorization-using-alternating-least-squares/1073170#1073170}}
  \item{\url{http://activisiongamescience.github.io/2016/01/11/Implicit-Recommender-Systems-Biased-Matrix-Factorization/}}
  \item{\url{http://datamusing.info/blog/2015/01/07/implicit-feedback-and-collaborative-filtering/}}
  \item{\url{https://jessesw.com/Rec-System/}}
  \item{\url{http://danielnee.com/2016/09/collaborative-filtering-using-alternating-least-squares/}}
  \item{\url{http://www.benfrederickson.com/matrix-factorization/}}
  \item{\url{http://www.benfrederickson.com/fast-implicit-matrix-factorization/}}
}
}
\keyword{datasets}
